fit <- multiRase(xtrain, ytrain, B1 = 100, B2 = 50, iteration = 0, base = 'lda', cores = 4)
#label from 1 to K
trans_inf <- labeltrans(ytrain)
ytrain <- trans_inf$vnew
lab_table <- trans_inf$label
xtrain <- as.matrix(xtrain)
# features of input
p <- ncol(xtrain)
n <- length(ytrain)
nmulti <- length(unique(ytrain))
# prior class
n_start <- rep(0,nmulti)
for (i in 1:nmulti){
n_start[i] <- sum(ytrain == i)
}
n_start
# clean data (delete variables with high collinearity and variables that have constant values)
a <- suppressWarnings(cor(xtrain))
b <- a - diag(diag(a)) # delete the elements with value 1
b0 <- which(abs(b) > 0.9999, arr.ind = T)
b0 <- b0[b0[, 1] > b0[, 2], ,drop = FALSE]
a <- diag(cov(xtrain))
delete.ind <- unique(c(which(a == 0), b0[, 1]))
sig.ind <- setdiff(1:p, delete.ind)
# estimate parameters
if (is.null(D)) {
D <- floor(min(sqrt(n), length(sig.ind)))
}
Sigma.mle <- 0
for(i in 1:nmulti){
Sigma.mle <- Sigma.mle + (n_start[i] - 1)*cov(xtrain[ytrain == i ,, drop = F])/n
}
mu.mle <- matrix(rep(0,p*nmulti),nmulti,p)
for(i in 1:nmulti){
mu.mle[i,] <- colMeans(xtrain[ytrain == i,,drop = F])
}
# start loops
dist <- rep(1, p)
dist[delete.ind] <- 0
for (t in 1:(iteration + 1)) {
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
if (is.matrix(output)) {
subspace <- output[, 3]
} else {
subspace <- output[3]
}
s <- rep(0, p)
for (i in 1:length(subspace)) {
s[subspace[[i]]] <- s[subspace[[i]]] + 1
}
dist <- s/B1
dist[dist < C0/log(p)] <- C0/p
dist[delete.ind] <- 0
}
iteration = 0
for (t in 1:(iteration + 1)) {
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
if (is.matrix(output)) {
subspace <- output[, 3]
} else {
subspace <- output[3]
}
s <- rep(0, p)
for (i in 1:length(subspace)) {
s[subspace[[i]]] <- s[subspace[[i]]] + 1
}
dist <- s/B1
dist[dist < C0/log(p)] <- C0/p
dist[delete.ind] <- 0
}
B1 = 20
B2 = 50
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
cv = 5
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
output[,1]
output[,1]
output[,2]
output[,3]
class(output)
class(output[1,3])
class(output[,3])
class(output[1,2])
B1 = 1
B2 = 1
if (is.matrix(output)) {
subspace <- output[, 3]
} else {
subspace <- output[3]
}
subspace
s <- rep(0, p)
for (i in 1:length(subspace)) {
s[subspace[[i]]] <- s[subspace[[i]]] + 1
}
s
length(subspace)
subspace
ytrain.pred
output[,2]
ytrain.pred <- data.frame(matrix(unlist(output[, 2]), ncol = B1))
y_count <- matrix(nrow = n,ncol = nmulti)
for(i in 1:n){
for(j in 1:nmulti){
y_count[i,j] = sum(ytrain.pred[i,] == j)/B1
}
}
y_count
n
nmulti
ytrain.pred <- data.frame(matrix(unlist(output[, 2]), ncol = B1))
ytrain.pred
y_count <- matrix(nrow = n,ncol = nmulti)
for(i in 1:n){
for(j in 1:nmulti){
y_count[i,j] = sum(ytrain.pred[i,] == j)/B1
}
}
y_count
output[,12]
output[1,2]
matrix(unlist(output[, 2])
)
matrix(unlist(output[1, 2]))
matrix(unlist(output[, 2]), ncol = B1)
matrix(unlist(output[1, 2]), ncol = B1)
unlist(output[, 2])
# 50 variables
set.seed(0, kind = "L'Ecuyer-CMRG")
p_seq <- rep(1/4,4)
train.data <- RaModel("multi_classification", 2, n = 200, p = 50, p0 = p_seq)
test.data <- RaModel("multi_classification", 2, n = 3000, p = 50, p0 = p_seq)
xtrain <- train.data$x
ytrain <- train.data$y
xtest <- test.data$x
ytest <- test.data$y
#label from 1 to K
trans_inf <- labeltrans(ytrain)
ytrain <- trans_inf$vnew
lab_table <- trans_inf$label
xtrain <- as.matrix(xtrain)
# features of input
p <- ncol(xtrain)
n <- length(ytrain)
nmulti <- length(unique(ytrain))
# prior class
n_start <- rep(0,nmulti)
for (i in 1:nmulti){
n_start[i] <- sum(ytrain == i)
}
# clean data (delete variables with high collinearity and variables that have constant values)
a <- suppressWarnings(cor(xtrain))
b <- a - diag(diag(a)) # delete the elements with value 1
b0 <- which(abs(b) > 0.9999, arr.ind = T)
b0 <- b0[b0[, 1] > b0[, 2], ,drop = FALSE]
a <- diag(cov(xtrain))
delete.ind <- unique(c(which(a == 0), b0[, 1]))
sig.ind <- setdiff(1:p, delete.ind)
# estimate parameters
if (is.null(D)) {
D <- floor(min(sqrt(n), length(sig.ind)))
}
D
Sigma.mle <- 0
for(i in 1:nmulti){
Sigma.mle <- Sigma.mle + (n_start[i] - 1)*cov(xtrain[ytrain == i ,, drop = F])/n
}
mu.mle <- matrix(rep(0,p*nmulti),nmulti,p)
for(i in 1:nmulti){
mu.mle[i,] <- colMeans(xtrain[ytrain == i,,drop = F])
}
# start loops
dist <- rep(1, p)
dist[delete.ind] <- 0
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
is.matrix(output)
output
xtrain
output[1,1]
output[,1]
output
B1
B2
B1  = 20
B2 = 50
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
s <- rep(0, p)
for (i in 1:length(subspace)) {
s[subspace[[i]]] <- s[subspace[[i]]] + 1
}
dist <- s/B1
dist[dist < C0/log(p)] <- C0/p
dist[delete.ind] <- 0
C0 = 0.1
dist <- s/B1
dist[dist < C0/log(p)] <- C0/p
dist[delete.ind] <- 0
for (t in 1:(iteration + 1)) {
output <- foreach(i = 1:B1, .combine = "rbind", .packages = "MASS") %dopar% {
S <- sapply(1:B2, function(j) {
S.size <- sample(1:D, 1)
c(sample(1:p, size = min(S.size, length(dist[dist != 0])), prob = dist),
rep(NA, D - min(S.size, length(dist[dist != 0]))))
}) # matrix of selected features (nrow = D, ncol = B2)
S <- sapply(1:B2, function(j) {
flag <- TRUE
while (flag) {
snew <- S[!is.na(S[, j]), j]
if (length(snew) > 2) {
ind0 <- findLinearCombos(Sigma.mle[snew, snew, drop = F])$remove
if (!is.null(ind0)) {
snew <- snew[-ind0]
}
}
snew1 <- c(snew, rep(NA, D - length(snew)))
if (sum(apply(mu.mle,1,var)) > 1e-10) {
flag <- FALSE
}
}
snew1
})
# Alternative of RaSubset
folds <- createFolds(ytrain, k = cv)
#library("MASS")
subspace.list <- sapply(1:B2, function(i) {
# the last row is training error for each i in 1:B2
Si <- S[, i][!is.na(S[, i])]  # current subspace
mean(sapply(1:cv, function(j) {
mean(predict(lda(x = xtrain[-folds[[j]], Si, drop = F], grouping = ytrain[-folds[[j]]]), xtrain[folds[[j]], Si, drop = F])$class !=
ytrain[folds[[j]]], na.rm = TRUE)
}))
})
i0 <- which.min(subspace.list)
S <- S[!is.na(S[, i0]), i0]  # final optimal subspace
xtrain.r <- xtrain[, S, drop = F]
fit <- lda(x = as.matrix(xtrain.r), grouping = ytrain)
ytrain.pred <- predict(fit, as.matrix(xtrain.r))$class
return(list(fit = fit, ytrain.pred = as.numeric(ytrain.pred), subset = S))
}
if (is.matrix(output)) {
subspace <- output[, 3]
} else {
subspace <- output[3]
}
# Get the frequency of each feature in B1 models served as the new dist
s <- rep(0, p)
for (i in 1:length(subspace)) {
s[subspace[[i]]] <- s[subspace[[i]]] + 1
}
dist <- s/B1
dist[dist < C0/log(p)] <- C0/p
dist[delete.ind] <- 0
}
output[,2]
y_count <- matrix(nrow = n,ncol = nmulti)
for(i in 1:n){
for(j in 1:nmulti){
y_count[i,j] = sum(ytrain.pred[i,] == j)/B1
}
}
y_count
output
nmulti
is.matrix(output)
？findLinearCombos
？findLinearCombos
?findLinearCombos
?sum
??sum
sum()
sum
View(Sigma.mle)
